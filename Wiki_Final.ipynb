{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part we load the dataset. We'll use the `nx.convert_matrix.from_pandas_edgelist` function to build the graph later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wikigraph_reduced.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dc0feb82be25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#loading Dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf_edges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wikigraph_reduced.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#making edge_ID as int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wikigraph_reduced.csv'"
     ]
    }
   ],
   "source": [
    "#dataframe for the network \n",
    "\n",
    "#names of columns\n",
    "cols = ['edge_ID','source','target']\n",
    "\n",
    "#loading Dataframe\n",
    "df_edges = pd.read_csv('wikigraph_reduced.csv', delimiter = '\\t',names = cols).drop(index = 0)\n",
    "\n",
    "#making edge_ID as int\n",
    "df_edges['edge_ID'] = list(map(int, df_edges['edge_ID']))\n",
    "\n",
    "#set of all the nodes in the graph\n",
    "nodes=set(df_edges[\"source\"]).union(set(df_edges[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection we load the dataset containing the names of the nodes.\n",
    "\n",
    "Since there is not a unique delimiter that separates the node number from the name and using the *\\\" \"* character doesn't work because it would also separates some names (e.g: *\\\"0 Chiasmal syndrome\"* would be split into three parts) we loaded everything into a simple column and then separated the node number from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.read_csv('wiki-topcats-page-names.txt.gz',names =['node name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names['node'] = [s.split()[0] for s in list(df_names['node name']) ]\n",
    "df_names['name'] = [' '.join(s.split()[1:]) for s in list(df_names['node name'])]\n",
    "df_names = df_names.drop('node name',axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection we load the dataset containing the categories of the nodes. We removed the prefix: *\\\"Category:\"* from each category name and grouped all the pages belonging to a certain category in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['cat_name','pages']\n",
    "df_cat = pd.read_csv('wiki-topcats-categories.txt.gz',sep = ';',names = names)\n",
    "\n",
    "df_cat['cat_name'] = df_cat.apply(lambda x: x['cat_name'][9:],axis = 1)\n",
    "\n",
    "df_cat['pages'] = df_cat.apply(lambda x: list(map(int,x['pages'].split())),axis = 1)\n",
    "\n",
    "df_cat['lens'] = df_cat.apply(lambda x : len(x['pages']),axis = 1)\n",
    "\n",
    "df_cat = df_cat.loc[(df_cat['lens']>5000) & (df_cat['lens']<30000)] #removing all the categories whose #pages are not in range (5000,30000)\n",
    "\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set of all nodes in df_cat \n",
    "nodes_reduced_df_cat = set(df_cat['pages'].sum())\n",
    "\n",
    "#check if all the nodes in the original database are a subset of the reduced category one\n",
    "nodes.issubset(nodes_reduced_df_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to remove the pages belonging to multiple categories, we made an inverted index key: Page, values: Categories in which the pages appears.\n",
    "#than I randomly sampled the values in uniform way and built the dataframe again\n",
    "\n",
    "categories = list(df_cat['cat_name'])\n",
    "pages_for_cat =list(df_cat['pages'])\n",
    "\n",
    "\n",
    "d_page_cat = defaultdict(list)\n",
    "for i,cat in enumerate(categories):\n",
    "    for pag in pages_for_cat[i]:\n",
    "        d_page_cat[pag].append(cat)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to remove the pages belonging to multiple categories we first created an inverted index with <code>key:Page</code>, <code>value:Categories in which the page appears</code>.\n",
    "\n",
    "Then, for each page we randomly sampled among the values in uniform way to only consider one category. Then we built the dataframe again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "categories = list(df_cat['cat_name'])\n",
    "pages_for_cat =list(df_cat['pages'])\n",
    "\n",
    "\n",
    "d_page_cat = defaultdict(list)\n",
    "for i,cat in enumerate(categories):\n",
    "    for pag in pages_for_cat[i]:\n",
    "        d_page_cat[pag].append(cat)\n",
    "    \n",
    "d_random_cat_page = defaultdict(list)\n",
    "\n",
    "for page in d_page_cat.keys():\n",
    "    if page in nodes:\n",
    "        cat_of_page = random.sample(set(d_page_cat[page]),1)\n",
    "        d_random_cat_page[cat_of_page[0]].append(page)\n",
    "\n",
    "#some categories have been removed, since there are no pages belonging to them or all their\n",
    "#pages were sampled into other categories\n",
    "\n",
    "new_df_cat = pd.DataFrame()\n",
    "new_df_cat['cat_name'] = d_random_cat_page.keys()\n",
    "new_df_cat['pages'] = list(d_random_cat_page.values())\n",
    "new_df_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we built an inverted dictionary with <code>key: node_i</code>,<code>value: category of node_i</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_dictionary={}\n",
    "for i,row in new_df_cat.iterrows():\n",
    "    for node in row[\"pages\"]:\n",
    "        inverted_dictionary[node]=row[\"cat_name\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1\n",
    "Build the graph $G=(V, E)$, where $V$ is the set of articles and E the hyperlinks among them. Then, provide its basic information:\n",
    "\n",
    "Is the graph directed?\n",
    "How many articles are we considering?\n",
    "How many hyperlinks between pages exist?\n",
    "Compute the average number of links in an arbitrary page. What is the graph density? Do you believe that the graph is dense or sparse? Is the graph dense?\n",
    "Visualize the nodes' degree distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the graph $G=(V, E)$, where $V$ is the set of articles and $E$ the hyperlinks among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.convert_matrix.from_pandas_edgelist(df_edges, 'source', 'target', ['edge_ID'],create_using=nx.DiGraph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Is the graph directed?\n",
    "\n",
    "Due the structure of the \"headers\" in the dataset (source-target) we must assume that the graph is directed. Indeed the object we are trying to model is the hyperlink network of the wikipedia pages and every link is one-sided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With this function we can double-check if the function we used effectively created a directed graph.\n",
    "G.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many articles are we considering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our network $G$ the articles are represented by the nodes. Therefore the amount of articles we're considering corresponds to the cardinality of the set of nodes in $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Hyperlinks ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $G$ the hyperlinks are represented by the edges. Therefore the amount of Hyperlinks corresponds to the cardinality of the set of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the graph is really far from being a dense graph: a densely connected graph with 98343 nodes would have 9671247306 edges while this one only has 483094."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_density=len(G.edges)/(len(G.nodes)*(len(G.nodes)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the average degree of the nodes. Since our graph is directed, we can interpret the question as both asking for the indegree or the outdegree of every node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_in_degree = dict(G.in_degree())\n",
    "list_degree_in = list(dict_in_degree.values())\n",
    "avg_degree_in = np.mean(np.array(list_degree_in))\n",
    "med_degree_in = np.median(np.array(list_degree_in))\n",
    "print(\"Mean of indegree:\", avg_degree_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_out_degree = dict(G.out_degree())\n",
    "list_degree_out = list(dict_out_degree.values())\n",
    "avg_degree_out = np.mean(np.array(list_degree_out))\n",
    "med_degree_out = np.median(np.array(list_degree_out))\n",
    "print(\"Mean of outdegree:\", avg_degree_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that these numbers are the same is not surprising: every edge has a target and a source. Since the sum over the nodes of their indegree is equal to the number of edges of the graph and the same as well as for the sum of the outdegree. Hence the average indegree and outdegree have the same value equal to the ratio between the cardinality of the edges and the nodes that we computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.edges)/len(G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remark that the distribution of the indegrees and outdegrees are usually different.\n",
    "\n",
    "As we can notice, the degree distribution is the typical one of *Scale Free Networks* in which we have few high-degree nodes and a lot of low-degee ones.\n",
    "Usually this phenomenun is a consequence of the \"preferential attachment\" property, for which new nodes added to the network are more likely to point to nodes that already have a lot of links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "g = sns.histplot(list_degree_in,bins = 100,color = '#96ded1')\n",
    "\n",
    "g.set_yscale('log')\n",
    "g.set_yscale('log')\n",
    "g.set_xlabel('Degree')\n",
    "g.set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "g.annotate('Asteroid belt', xy=(10750.28, 1), xytext=(10650, 4),\n",
    "            arrowprops=dict(facecolor='black'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "g = sns.histplot(list_degree_in,bins = 100,color = '#a12655')\n",
    "\n",
    "g.set_yscale('log')\n",
    "g.set_yscale('log')\n",
    "g.set_xlabel('Degree')\n",
    "g.set_ylabel('Frequency')\n",
    "\n",
    "g.set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list_degree_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.hist(list_degree_out,log=\"xy\",bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that takes in input a page $v$ and a number of clicks $d$ and returns the set of all pages that a user can reach within $d$ clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use a standard iterative BFS with a queue to avoid having too many recursive calls\n",
    "\n",
    "def BFS_distances(G,v,n_clicks=float(\"inf\")):\n",
    "\n",
    "    \"\"\"G: graph\n",
    "    v: starting node\n",
    "    n_clicks: optional parameter. Returns all pages with distance less or equal han n_clicks. Base value is 'float(\"Inf\"):\n",
    "    it returns every page reached by the BFS.\"\"\"\n",
    "    \n",
    "    q = deque() # initialize empty queue\n",
    "    discovered={} # dictionary with already discovered nodes. The structure is key:Node, value=distance from source\n",
    "    \n",
    "    \n",
    "    discovered[v] = 0 # distance of the source from the source is 0 \n",
    "    q.append(v)   # put starting vertex in the queue\n",
    "    \n",
    "    edges_with_distances=[] #list of tuples (node,distance from source) that will be returned.\n",
    "    \n",
    "    while q: #While the queue is not empty, do an iteration of BFS\n",
    "        v = q.popleft()     #remove first item in the queue\n",
    "        if discovered[v]>n_clicks:\n",
    "            break\n",
    "        edges_with_distances.append((v,discovered[v]))\n",
    "        \n",
    "        \n",
    " \n",
    "        for u in G.neighbors(v):\n",
    "        # for each neighbor u of v. we check if u has already been discovered. If not, we know that its distance of u from the\n",
    "        # source is one more than the distance v has. \n",
    "            \n",
    "            if u not in discovered: \n",
    "                # mark it as discovered and push it into queue\n",
    "                discovered[u] = discovered[v]+1\n",
    "                q.append(u)\n",
    "                \n",
    "    return edges_with_distances\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFS_distances(G,67030,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that takes in input: a category $C$ and aset of pages in $C$, $p = {p_1, ..., p_n}$\n",
    "and returns the minimum number of clicks required to reach all pages in $p$, starting from the page $v$, corresponding to the most central article, according to the in-degree centrality, in $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first add the \"Category\" as an attribute to every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, inverted_dictionary, name=\"Category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to use the in_degree as our metric for the \"most central article\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_central_article(G,category):\n",
    "    \n",
    "    \"\"\" G: graph\n",
    "        category: one of the categories\n",
    "        \n",
    "        returns the node with the highest in_degree as a tuple: (node, in_degree)\"\"\"\n",
    "    \n",
    "    nodes = list(map(int, d_random_cat_page[category] ))\n",
    "    assert len(nodes)>0, \"Empty/non-existent category!\"\n",
    "    max_in_degree = list(G.in_degree(nodes))\n",
    "    return max( max_in_degree, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_central_article(G,'Debut_albums')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm we will use to solve this problem will have two steps:\n",
    "First step: we use the function <code>graph_exploration</code> to build a new graph that only has the nodes in the list $p$\n",
    "and the most central article in the graph. An edge from $u$ to $v$ is added whenever there exists a path from these two nodes in the original graph and the weight of this edge will be equal to the lenght of the shortest path between the two.\n",
    "\n",
    "Second part: we look for the shortest path that visits all nodes in the new graph that we build in the first step. This is an asymmetric travelling salesman problem (with a slight modification: one does not need to get back to the starting point) with distances satisfying the triangle inequality. We decide to use a slight modification of the \"nearest neighbor\" algorithm to give an approximation of the solution in a polynomial time: that algorithm can easily answer \"no existing path\" when there are nodes that are not connected to all the others. Since in our case we know that if $u$ is connected to $v$ and $v$ is connected to $w$ then $u$ is also connected to $w$ (this can be seen as a consequence of the fact that the triangle inequality has to hold and by putting $dist(A,B)=+\\infty$ when a directed edge from $A$ to $B$ does not exist). With this property in mind, we know that if we do not see all the unvisited edges in the neighbors of a particular node, than picking that node will lead to a path that doesn't visit all the pages. We try to avoid this by first looking if there is a node that has all the remaining nodes to visit among its neighbors.\n",
    "\n",
    "\n",
    "(N.B It is interesting to observe how this the \"shortest path between two nodes\" does not define a distance in the classical sense since it's not symmetric in a generic directed graph with weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_exploration(G,p,C):\n",
    "    \"\"\"G: graph\n",
    "     p: list of nodes  to visit\n",
    "     C: category where the most central article will be selected as a starting point.\"\"\"\n",
    "    \n",
    "    most_central=most_central_article(G,C)[0] #we compute the most central node\n",
    "    \n",
    "    if most_central not in p: #this is here in case one already had the most central node in p\n",
    "        p.append(most_central) \n",
    "    \n",
    "    new_graph=nx.empty_graph(create_using=nx.DiGraph()) #this creates a new empty graph\n",
    "    \n",
    "    for node1 in p:\n",
    "        for node2 in p:\n",
    "            if node1!=node2:\n",
    "                distances=dict(BFS_distances(G,node1)) #we calculate the distances of all the nodes in the graph G from node1\n",
    "                if node2 in distances: #if node 2 was reached by a BFS from node1, we add an edge in the new graph.\n",
    "                    new_graph.add_edge(node1,node2,weight=distances[node2])\n",
    "    new_graph.add_nodes_from(p) #We finally add all the nodes from p, in case there was at least one node that was not\n",
    "                                #connected to any other in the list\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[1184017,903941,1056184] #The assignment asks for these to be picked in the C category. Our algorithm does not require it,\n",
    "                           #but it can be done\n",
    "C='English_footballers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_graph=graph_exploration(G,p,C) #this is the execution of step 1 of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.get_edge_attributes(new_graph,\"weight\") #checking the edges in the newly created graph with their associated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a slightly modified version of the greedy nearest neighbor algorithm.\n",
    "\n",
    "def TSP(new_graph,start_node=most_central_article(G,C)[0]):\n",
    "    \n",
    "    remaining_nodes=set(new_graph.nodes()) #nodes still not visited\n",
    "    path=[start_node] #path starts from the startnode\n",
    "    remaining_nodes.remove(start_node) #we remove the startnode since it has already been visited\n",
    "    \n",
    "    current_node=start_node\n",
    "    \n",
    "    while (current_node!=None):  #We loop until we cannot find new nodes\n",
    "        successors=sorted(new_graph.out_edges(current_node,data=True),key=lambda x:x[2][\"weight\"]) #we sort the outgoing edges\n",
    "        #of the current node by weight\n",
    "        \n",
    "        current_node=None  #since the current node has already been visited, we set it to \"None\" and start looking for the next one.\n",
    "        \n",
    "        for edge in successors: #the successors are ranked by the weight\n",
    "            node=edge[1] #this is the target of the edge.\n",
    "            \n",
    "            #the following if checks if it is still possible to visit all the remaining vertices from this if it is the next node.\n",
    "            #if it is, the for loop breaks, and the program goes back into the while loop for another iteration.\n",
    "            if remaining_nodes.difference(set(new_graph.successors(node)))==set([node]):\n",
    "                current_node=node\n",
    "                path.append(node)\n",
    "                remaining_nodes.remove(node)\n",
    "                break\n",
    "    \n",
    "    #if a new node for the path was not found, we check if we completed the path or not.\n",
    "    if remaining_nodes:\n",
    "        print(\"Cammino non trovato\")\n",
    "        return None\n",
    "    \n",
    "    #The following lines are here only for a nice output format.\n",
    "    edges_of_path=[(path[i],path[i+1],\"weight: \"+str(nx.get_edge_attributes(new_graph,\"weight\")[(path[i],path[i+1])])) for i in range(len(path)-1)]\n",
    "    weights=sum(nx.get_edge_attributes(new_graph,\"weight\")[edge[:2]] for edge in edges_of_path)\n",
    "    print(\"The total lenght of the minimum path is: \",weights)\n",
    "    print(edges_of_path)       \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSP(new_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given in input two categories: $C_1$ and $C_2$, we get the subgraph induced by all the articles in the two categories.\n",
    "\n",
    "Let $v$ and $u$ two arbitrary pages in the subgraph. What is the minimum set of hyperlinks one can remove to disconnect $u$ and $v$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category1='Association_football_defenders'\n",
    "Category2='English_footballers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the list of nodes in one of these categories and then build a subgraph made of those nodes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_c1c2 = [x for x,y in G.nodes(data=True) if y['Category']==Category1 or y['Category']==Category2]\n",
    "H=G.subgraph(nodes_c1c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that returns the minimum number of values. We took inspiration from the Ford-Fulkerson algorithm (with integer values as capacities) and from the fact that the maxflow in a graph equals the mincut. This is actually a simpler version: every edge has \"capacity\" one, so a simpler variant of the algorithm could simply try to find the target $T$ from the source $S$, then try to find another path that uses completely different edges. The number of total distinct path corresponds to the mincut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mincut(G, S, T):\n",
    "    \n",
    "    #This is a simpler variant of the maxflow problem.\n",
    "    #We initialize the max_flow to 0 and assume a path exists\n",
    "    assert S!=T,\"Source and target cannot be the same!\"\n",
    "    \n",
    "    _mincut=0\n",
    "    maybe_path_exists=True\n",
    "    already_used_edges=set()\n",
    "        \n",
    "    while maybe_path_exists: #While a path could exists, a BFS starts\n",
    "        predecessor={}\n",
    "        predecessor[S] = S\n",
    "        discovered=predecessor.keys()\n",
    "        \n",
    "        #WARNING: the two lines above are actually using a feature in Python that is not liked by every programmer:\n",
    "        #\"discovered\" is NOT a frozen copy of the predecessory keys, but is something that will change whenever\n",
    "        #a new item is added to the \"predecessor\" dictionary. If one is familiar with C/C++, this is similar to having \n",
    "        #two pointers pointing to the same parts of memory. \n",
    "        \n",
    "        \n",
    "        # we need to do a BFS from S looking for T. We cannot use edges already used in previus searches.\n",
    "        q = deque()\n",
    "        \n",
    "        q.append(S) #We push S into the queue to start the BFS\n",
    "        while q: #While the queue is not empty\n",
    "            v = q.popleft()\n",
    "            for u in G.neighbors(v):\n",
    "                if (u not in discovered) and ((v,u) not in already_used_edges):\n",
    "                    # mark it discovered and push it into queue\n",
    "                    predecessor[u] = v #read the warning above: this also modifies the \"discovered\" array!\n",
    "                    q.append(u)\n",
    "                    if (T in discovered): #if the target was discovered, we can add 1 to the mincut, check the path\n",
    "                        #we used and mark them as \"already used\" and break both the \"for\" loop and the \"while q\" loop to \n",
    "                        #start a new search.\n",
    "                        _mincut+=1\n",
    "                        node=T\n",
    "                        while predecessor[node]!=node:\n",
    "                            already_used_edges.add((predecessor[node],node))\n",
    "                            node=predecessor[node]\n",
    "                        q=[]\n",
    "                        break\n",
    "                        \n",
    "        if (T not in discovered): #if T was not discovered, we found the mincut to return. It can be 0 if u and v \n",
    "            #are not connected\n",
    "            maybe_path_exists=False\n",
    "    return (_mincut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mincut(H,1357914,82385)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. The subgraph containing only the nodes of two categories is very sparsely connected, even more so than the original subgraph. The function returns 0 a lot when tested in the subgraph, while it's easier to get a non-zero value in the original graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that, given an arbitrary category $C_0$ as input, returns the list of remaning categories sorted by their distance from $C0$. In particular, the distance between two categories is defined as\n",
    "\n",
    "distance($C_0$, $C_i$) = median(ShortestPath($C_0$, $C_i$))\n",
    "\n",
    "where ShortestPath($C_0$,$ C_i$) is the set of shortest paths from each pair of nodes in the two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category0='Association_football_defenders'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the distance of all the nodes starting from each node of the $C_0$ category (that we called <code>Category_0</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=list(map(int,d_random_cat_page[Category0]))\n",
    "all_distances={}\n",
    "for v in tqdm(nodes):\n",
    "    distances=BFS_distances(G,v) #This function calculates the distance of every node in G from the source v.\n",
    "    all_distances[v]=dict(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the distances among all tuples ($C_0$, $C_i$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians=[]\n",
    "\n",
    "for i in tqdm(d_random_cat_page):\n",
    "    temp_nodes=list(map(int,d_random_cat_page[i]))\n",
    "    distances=[]\n",
    "    \n",
    "        \n",
    "    for c_0 in nodes: #we iterate over all nodes in the Category_0\n",
    "        for c_i in temp_nodes: #and over all nodes in all the Categories.\n",
    "            try:\n",
    "                distances.append(all_distances[c_0][c_i]) #if a distance is found in the previous calculation, add it;\n",
    "            except KeyError:\n",
    "                distances.append(float(\"inf\")) # else, we put \"infinity\" as the distance.\n",
    "    \n",
    "    medians.append((i,np.median(np.array(distances)))) #We finally calculate all the medians\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians=sorted(medians,key=lambda x:x[1]) #We sort the array by the medians value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to also check the median of the distance among nodes in the same category, so all 21 categories will be here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ 6\n",
    "\n",
    "Write a function that sorts the categories in the graph according to their PageRank (PR). For this task you need to model the network of categories such that you can apply the PR algorithm.\n",
    "\n",
    "\n",
    "#### Building the DataFrame\n",
    "In order to accomplish this task, we built a graph in the following way:\n",
    "* each node represent a category\n",
    "* the node associated to the category_1 points the node of the category_2 if there is at least one page in category_1 that points a page in category_2\n",
    "\n",
    "In order to build this network we replaced all the pages in the dataframe <code>edges</code> with their respective categories, by mean of <code>cat_of_page</code> dictionary, which returns for every page, the category to which it belongs.\n",
    "Then we converted the dataframe into a Graph Object with the same procedure applied above.\n",
    "\n",
    "#### The Page Rank Score\n",
    "First we modify a little the network, adding a link to every other node from the nodes whose out-degree is zero. \n",
    "\n",
    "Then we build the network's matrix $p$ ,  $p_{ij}=  d * \\frac{1}{|N|} + (1-d)* 1_{\\{(i,j)\\}}*\\frac{1}{\\delta^+(n_i)}$. \n",
    "Where:\n",
    "* $N$ = Set of the nodes\n",
    "* $d$ = *dumping factor*, probability that the web-surfer goes to the next category through the search bar (instead of using a link of the page). Default = 0.85.\n",
    "* $1_{\\{(i,j)\\}} = 1$ if $\\exists$ edge $(i,j)$  , $0$ else\n",
    "* $\\delta^+(n_i)$ is the *out-degree* of the node \n",
    "\n",
    "The Page Rank vector is an enginevector of enginevalue 1 of $p$. Therefore, if we start from a random page (e.g. the first one) $q_0$, and then we iterate $q_{(t+1)} = p* q_{(t)}$, eventually we will converge to it after a finite number of steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the Data Frame of Categories\n",
    "cat_of_page = {}\n",
    "\n",
    "for cat in d_random_cat_page: #d_random_cat_page returns for each category, the list of the pages belonging to it\n",
    "    for page in d_random_cat_page[cat]: # in cat_of_pages we assign to each page in\n",
    "        cat_of_page[page] = cat         # d_random_cat_page[cat], the category as value\n",
    "                                        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges_cat = pd.DataFrame()\n",
    "df_edges_cat['source'] = [cat_of_page[page] for page in df_edges['source']]\n",
    "df_edges_cat['target'] = [cat_of_page[page] for page in df_edges['target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges_cat #the nodes are replaced with the categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_cat=nx.convert_matrix.from_pandas_edgelist(df_edges_cat, 'source', 'target',create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def PageRank(G,alpha=0.85,max_iter=100,epsilon=1e-6):\n",
    "    d_node = {node: i for i,node in enumerate(list(G.nodes()))}  #map each node with a number\n",
    "    \n",
    "    n = len(G.nodes()) \n",
    "    \n",
    "    # p = d*uniform_p + (1-d)* outlink_p\n",
    "    \n",
    "    uniform_p = np.full((n,n),1/n)  #uniform_p = matrix  1/n * I , dim(I) = n X n \n",
    "    \n",
    "    outlink_p = np.zeros((n,n))  #outlink_p[i,j] = 1/deg(i) *1_{(i,j) exists}\n",
    "    for node in G.nodes():\n",
    "        i = d_node[node]\n",
    "        for neigh in G.successors(node):\n",
    "            j = d_node[neigh]\n",
    "            outlink_p[i][j] = 1/len(list(G.successors(node)))\n",
    "    \n",
    "    #adding an edge to each node for those nodes whose out-degree = 0         \n",
    "    for i in range(n):\n",
    "        if np.sum(outlink_p[i])==0:\n",
    "            outlink_p[i] = np.full((1, n), 1/n) \n",
    "    \n",
    "    p = (1-alpha)*uniform_p+(alpha)*outlink_p\n",
    "    \n",
    "    q = np.zeros((1,n)) \n",
    "    \n",
    "    q[0][0] = 1  #we start from the first category\n",
    "    \n",
    "    def distance(A,B):\n",
    "        C = np.abs(A-B)\n",
    "        return np.sum(C)\n",
    "    \n",
    "    for i in tqdm(range(max_iter)):\n",
    "        q_new = np.dot(q,p) #q_(t+1) = q_(t) *p \n",
    "        \n",
    "        if epsilon: #establish a criteria for the convergence, if the q's of two consecutive steps are close -> stop.\n",
    "            if distance(q,q_new) < epsilon:\n",
    "                break\n",
    "        \n",
    "        q = q_new\n",
    "\n",
    "    d = {node: q[0][d_node[node]] for node in G.nodes()} #for each category, we indicate the page rank score\n",
    "    \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we report the PageRank score computed by Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_nx_tuples = nx.pagerank(G_cat,alpha = 0.85,max_iter= 100).items()\n",
    "\n",
    "PR_nx = pd.DataFrame(PR_nx_tuples,columns=['Category','Page Rank']).sort_values(['Page Rank','Category'],ascending = False)\n",
    "\n",
    "PR_nx \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_tuples = PageRank(G_cat,epsilon = False).items()\n",
    "\n",
    "PR = pd.DataFrame(PR_tuples,columns=['Category','Page Rank']).sort_values(['Page Rank','Category'],ascending = False)\n",
    "\n",
    "PR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_dict  = PageRank(G_cat,epsilon = False)\n",
    "\n",
    "PR_tuples = PR_dict.items()\n",
    "\n",
    "PR = pd.DataFrame(PR_tuples,columns=['Category','Page Rank']).sort_values(['Page Rank','Category'],ascending = False)\n",
    "\n",
    "colors = [PR_dict[n] for n in G_cat.nodes()]\n",
    "sizes = [PR_dict[n]*10000 for n in G_cat.nodes()]\n",
    "plt.figure(figsize=(20,18))\n",
    "\n",
    "\n",
    "# drawing nodes and edges separately so we can capture collection for colobar\n",
    "pos = nx.kamada_kawai_layout(G_cat)\n",
    "ec = nx.draw_networkx_edges(G_cat, pos, alpha=0.1,arrowsize=20)\n",
    "nc = nx.draw_networkx_nodes(G_cat, pos, nodelist=G_cat.nodes(), node_color=colors, \n",
    "                            with_labels=True, node_size=sizes, cmap=plt.cm.magma)\n",
    "pos_higher = {}\n",
    "y_off = 0.03  # offset on the y axis\n",
    "\n",
    "for k, v in pos.items():\n",
    "    pos_higher[k] = (v[0], v[1]+y_off)\n",
    "\n",
    "\n",
    "lc = nx.draw_networkx_labels(G_cat, pos_higher, font_size=17)\n",
    "\n",
    "plt.colorbar(nc)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
